{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08238e7b",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-icons-png.flaticon.com/512/784/784856.png\" alt=\"Telecom Churn\" style=\"width: 70px; float:left; margin-right:8px\"><b><p style='font-size:24px'><font color='#808080'>Agenda - Revision Topics</font></p></b></img>\n",
    "\n",
    "---\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb79e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "* Supervised Learning\n",
    "* Allows to make prediction from labelled data, if the target variable is `continous`\n",
    "\n",
    "\n",
    "`Linear Regression` is a statistical model used to predict the relationship between independent and dependent variables\n",
    "* Basically we examine 2 factors\n",
    "    * Which variables in particular are significant predictors of the outcome variables?\n",
    "    * How significant is the `regression line` to make predictions with highest possible accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Linear Regression (SLR)\n",
    "\n",
    "Equation of line,\n",
    "\n",
    "${y} = {mx} + {c}$\n",
    "\n",
    "\n",
    "SLR Equation,\n",
    "\n",
    "${\\hat y} = {\\beta_0} + {\\beta_1x}$\n",
    "\n",
    "where,\n",
    "\n",
    "* ${\\hat y}$ is dependent variable or target variable\n",
    "* x is independent variable or predictor variable\n",
    "\n",
    "\n",
    "Residuals or prediction error,\n",
    "\n",
    "${error} = {y_{actual}} - {y_{prediced}}$\n",
    "\n",
    "\n",
    "total error $ = {\\epsilon_1^2} + {\\epsilon_2^2} + {\\epsilon_3^2} + ... + {\\epsilon_n^2}$\n",
    "\n",
    "total error $ = {\\sum_{i=1}^n\\epsilon_i^2}$\n",
    "\n",
    "total error $ = {\\sum_{i=1}^n({y_i} - {\\hat y})^2}$\n",
    "\n",
    "total error $ = {\\sum_{i=1}^n({y_i} - ({\\beta_0} + {\\beta_1x}))^2}$\n",
    "\n",
    "Now, this total error is also known as `Residual Sum of Squares or RSS`\n",
    "\n",
    "RSS $ = {\\sum_{i=1}^n({y_i} - ({\\beta_0} + {\\beta_1x}))^2}$\n",
    "\n",
    "$({\\sum_{i=1}^n({y_i} - ({\\beta_0} + {\\beta_1x}))^2}) \\Longrightarrow$ `cost function`\n",
    "\n",
    "Note: we need to minimize the `cost function` by finding ideal values for ${\\beta_0}$ and ${\\beta_1}$, by using `Gradient Descent` optimization algorithm. As part of this optimization process the system runs various iteration of ${\\beta_0}$ and ${\\beta_1}$ and then finds the `best fit line` using `cost function`. \n",
    "\n",
    "This method is also known as `OLS` or `Ordinary Least Square` method\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple Linear Regression (MLR)\n",
    "\n",
    "In case of `Multiple Linear Regression`, the underlying concept remains same however this time we will use more than 1 predictors and equation of MLR looks like this,\n",
    "\n",
    "${\\hat y} = {\\beta_0} + {\\beta_1x_1} + {\\beta_2x_2} + {\\beta_3x_3} + {\\beta_4x_4} + {\\beta_5x_5} + ... + {\\beta_nx_n}$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### R Squared\n",
    "<img src=\"r-square-diagram.png\" alt=\"R Square\" style=\"width: 500px;float:center;\"></img>\n",
    "\n",
    "$${R^2} = \\frac{Explained Variance}{Total Variance} = {1} - \\frac{Unexplained Variance}{Total Variance} = {1} - \\frac{RSS}{TSS}$$\n",
    "\n",
    "Where `TSS` is `Total Sum of Squares` basically sum of diffence between mean value and actual data point\n",
    "\n",
    "<img src=\"r-square-ols.png\" alt=\"Sample OLS Model\" style=\"width: 500px;float:center;\"></img>\n",
    "\n",
    "If we are able to explain more deviation the better will be our ${R^2}$ value. \n",
    "\n",
    "Note: Values of ${R^2}$ lies between 0 and 1, we should target our model ${R^2}$ closer to 1 i.e. try to explain maximum or 100% deviation\n",
    "\n",
    "\n",
    "* The coefficient of determination (${R^2}$) is key output of regression analysis\n",
    "* ${R^2}$ is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable\n",
    "* It helps to determine the predictive power of a regression model\n",
    "\n",
    "$${R^2} = {1} - \\frac{\\sum({y} - {\\hat y})^2}{\\sum({y} - {\\bar y})^2}$$\n",
    "\n",
    "**Important Points**\n",
    "* Adding new predictors to model will either increase ${R^2}$ or it will remain same\n",
    "* When ever we are adding predictors to model, we are increasing the complexity of the model as well\n",
    "* With increase in model complexity, model tends to overfit which is not desired\n",
    "* To handle this, our algorithm penalizes the model for using more predictors and make is more complex\n",
    "* The score after adding regularization to model is known as adjusted ${R^2}$\n",
    "\n",
    "$${R^2_{adjusted}} = {1} - \\frac {({1} - {R^2}) ({N} - {1})} {{N} - {p} - {1}}$$\n",
    "\n",
    "where,\n",
    "* ${R^2}$ = sample R-square\n",
    "* p = number of predictors\n",
    "* N = total sample size\n",
    "\n",
    "\n",
    "### Feature Selection\n",
    "* Try all possible combination of features to build model\n",
    "* Manual feature elimination (Need Business Knowledge)\n",
    "    * Build model\n",
    "    * Drop features that are least helpful in prediction (high p-value)\n",
    "    * Drop features that are redundant (using correlations, VIF (Variation Inflation Factor)) -> Multicollinearity\n",
    "    * Rebuild model and repeat\n",
    "* Automated approach\n",
    "    * Top 'n' features: RFE (Recursive Feature Elimination)\n",
    "    \n",
    "What is Multicollinearity?\n",
    "* Presence of co-related independent variables in the regression model\n",
    "\n",
    "${\\hat y} = {\\beta_0} + {\\beta_1x_1} + {\\beta_2x_2} + {\\beta_3x_3} + {\\beta_4x_4} + {\\beta_5x_5} + ... + {\\beta_nx_n}$\n",
    "\n",
    "General expectation in linear regression model is that, if we increase $x_1$ by one unit and keep ${x_2},{x_3},...,{x_n}$ constant, the target variable ${\\hat y}$ will be impacted by $\\beta_1$. But when $x_1$ and $x_2$ are so much correlated with each other we can not keep $x_2$ constant while changing $x_1$ and because of this the co-efficients become non-interpretable.\n",
    "\n",
    "In case of `Multicolinearity`,\n",
    "* Beta sign will change arbitrarily (if $x_1$ = - $x_2$)\n",
    "* Increase model complexity\n",
    "\n",
    "To handle `Multicolinearity` problem, we can either check correlation or use VIF to eliminate highly correlated predictors,\n",
    "\n",
    "\n",
    "$${VIF} = \\frac{1}{{1} - {R^2}}$$\n",
    "\n",
    "**Important Point**: This is not the ${R^2}$ of the model but the ${R^2}$ of the various models created using only independent variables. let's say we have model using three predictors as,\n",
    "\n",
    "${\\hat y} = {\\beta_0} + {\\beta_1x_1} + {\\beta_2x_2} + {\\beta_3x_3}$\n",
    "\n",
    "|Model Number| Model | $${R^2}$$|$${VIF} = \\frac{1}{{1} - {R^2}}$$|Status|\n",
    "|---|---|---|---|---|\n",
    "|1|$${x_1} = {\\alpha_0} + {\\alpha_1x_2} + {\\alpha_2x_3}$$|10%|1.1||\n",
    "|2|$${x_2} = {\\alpha_0^{'}} + {\\alpha_1^{'}x_1} + {\\alpha_2^{'}x_3}$$|20%|1.25||\n",
    "|3|$${x_3} = {\\alpha_0^{''}} + {\\alpha_1^{''}x_1} + {\\alpha_2^{''}x_2}$$|80%|5||\n",
    "\n",
    "How to interpret these models,\n",
    "* We can say that for `model 3`, $x_1$ and $x_2$ together can explain `80%` of the variance of $x_3$\n",
    "* Which means that we can predict $x_3$ from $x_1$ and $x_2$, so $x_3$ can be considered as `redundant` and can be dropped from the model\n",
    "\n",
    "**Important Point**: Any variable with `VIF >= 5` is considered as highly multicolinear and can be dropped from model\n",
    "\n",
    "|Model Number| Model | $${R^2}$$|$${VIF} = \\frac{1}{{1} - {R^2}}$$|Status|\n",
    "|---|---|---|---|---|\n",
    "|1|$${x_1} = {\\alpha_0} + {\\alpha_1x_2} + {\\alpha_2x_3}$$|10%|1.1|Keep|\n",
    "|2|$${x_2} = {\\alpha_0^{'}} + {\\alpha_1^{'}x_1} + {\\alpha_2^{'}x_3}$$|20%|1.25|Keep|\n",
    "|3|$${x_3} = {\\alpha_0^{''}} + {\\alpha_1^{''}x_1} + {\\alpha_2^{''}x_2}$$|80%|5|Drop|\n",
    "\n",
    "Final model,\n",
    "\n",
    "${\\hat y} = {\\beta_0} + {\\beta_1x_1} + {\\beta_2x_2}$\n",
    "\n",
    "**Important Point**: If we have more than one predictor with `VIF >= 5`, drop them one by one, never ever drop multiple predictors at once, because once we drop one predictor this whole process of VIF is re-calculated and probably that might give us diffrent VIF values for predictors.\n",
    "\n",
    "#### Recursive Feature Elimination\n",
    "* Elimination is based on the magnitude of the coefficients\n",
    "    * Retain features with high coefficients\n",
    "    * Eliminate features with low coefficients\n",
    "    \n",
    "**RFE - Backward Elimination Technique**\n",
    "\n",
    "Let's assume we have model with 6 predictors, now we need to select 3 features, so this is how the features will be eliminated,\n",
    "* Create model will all the features, ${\\hat y} = {\\beta_0} + {\\beta_1x_1} + {\\beta_2x_2} + {\\beta_3x_3} + {\\beta_4x_4} + {\\beta_6x_6} + {\\beta_6x_6}$. Now let'say $\\beta_3$ is the smallest.\n",
    "* RFE will eliminate the $x_3$ feature and re-create the model with remaining features\n",
    "* Now this process will repeat untill be get we reach the desired level of feature selection i.e. 3 in this case\n",
    "\n",
    "**Important Point:** Even after selecting features from RFE, we still need to check VIF and correlation for predictor as RFE doen not guarantee solving multicolinearity problem. Most of the time we use `mixed approach` i.e. select features using RFE and then eliminate features manually based on domain knowledge, VIF, p-values etc. \n",
    "\n",
    "### Hypothesis and P Value (F-Statistic): All about model validity\n",
    "<img src=\"p-values.png\" alt=\"Sample OLS Model\" style=\"width: 500px;float:center;\"></img>\n",
    "\n",
    "\n",
    "Let's see one scenario, assume a `professor` has 4 teaching assistants namely `A, B, C and D`, every `monday` professor used to pick one of them on random to help him with his work for entire week. After few weeks he asked his  assistants to write their name on a chit and drop that in a box where he will pick one chit at random and which every name is picked, that assitant will do his work for that week. Everyone agreed to this process and performed this activity on week 1 monday, `A's` name was not picked. Then comes week 2 monday, again `A's` name was not picked, similarly for straight 12 weeks `A's` name was not picked. After this 12 week, assitant's started doubt `A's` honesty and decided to open all the chits.\n",
    "\n",
    "<img src=\"p-value-intuation.png\" alt=\"Sample OLS Model\" style=\"width: 500px;float:center;\"></img>\n",
    "\n",
    "In this case let's understand about the `Null Hypothesis`.\n",
    "\n",
    "Null Hypothesis ($H_0$): A is honest (Status Quo)\n",
    "\n",
    "Alternate Hypothesis ($H_1$): A is not honest\n",
    "* Week 1, probability of `A's` name not picked was `.75` which was exceptable to all and they were `failed to reject Null Hypothesis`\n",
    "* Week 2, probability of `A's` name not picked was `.56` which was exceptable to all and they were `failed to reject Null Hypothesis`\n",
    "* Week 3, probability of `A's` name not picked was `.42` which was exceptable to all and they were `failed to reject Null Hypothesis`\n",
    "* Week 4, probability of `A's` name not picked was `.32` which was exceptable to all and they were `failed to reject Null Hypothesis`\n",
    "* Before looking into probability for `A's` name was not picked in week 12, it's very unlikely that some person name is not picked for straight 12 weeks. Now probability of `A's` name not picked in week 12 is 0.03, which is not at all fall in acceptable limit hence they decided to `reject the null hypothesis` and `accept alternate hypothesis` i.e. they doubted `A's` honesty and decided to open the chits.\n",
    "\n",
    "This probability of `A's` name not picked is called `p-value`\n",
    "* If p-value is high (>= 0.05), we fail to reject `null hypothesis`\n",
    "* If p-value is low (>= 0.05), we reject `null hypothesis`, change from status quo is significant\n",
    "\n",
    "**Important Point:** P Low, Null Go\n",
    "\n",
    "In case of Linear Regression our null and alternate hypothesis is as,\n",
    "\n",
    "Null Hypothesis ($H_0$): Our predictors can not predict the output variable or there is not linear relationship between the predictors and target variable (Status Quo). Basically, ${\\beta_1} + {\\beta_2} + {\\beta_3} + {\\beta_4} + ... + {\\beta_n} = {0}$\n",
    "\n",
    "Alternate Hypothesis ($H_1$): Atleast one ${\\beta_i} \\neq {0}$\n",
    "\n",
    "In statistic we call it `ANOVA` test.\n",
    "\n",
    "#### P>|t| Value\n",
    "\n",
    "Null Hypothesis ($H_0$): $\\beta_1$ = 0\n",
    "\n",
    "Alternate Hypothesis ($H_1$): ${\\beta_1} \\neq {0}$\n",
    "\n",
    "* `P>|t| < 0.05`, significant feature, else drop that feature\n",
    "* However we get all `P>|t| < 0.05`, we still need to check `VIF` values for model predictors\n",
    "\n",
    "**Important Point:** In SLR, $R^2$ = $r^2$ (correlation coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246f8b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Logistic Regression\n",
    "* Extension of `Linear Regression`\n",
    "* Used for `classification`\n",
    "    * Binary\n",
    "        * 0 - no, good, non-churn\n",
    "        * 1 - yes, bad, churn\n",
    "    * Multiclass\n",
    "        * 0\n",
    "        * 1\n",
    "        * 2\n",
    "\n",
    "output of `linear regression` (${\\hat y} = {\\beta_0} + {\\beta_1x}$) $\\Longrightarrow transformation \\Longrightarrow$ output of `logistic regression` (probability: 0 or 1) \n",
    "\n",
    "**Important Point:** this `transformation` is called `sigmoid transformation` or `logit transformation`\n",
    "\n",
    "$$P(0 or 1) = \\frac{1}{{1} + {{e}^{-({\\beta_0} + {\\beta_1x})}}}$$\n",
    " \n",
    " \n",
    "### Log Odds\n",
    "\n",
    "${{odds} = \\frac {P}{{1} - {P}}}$\n",
    "\n",
    "For example,\n",
    "\n",
    "${{odds(win)} = \\frac {0.8}{{1} - {0.2}} = \\frac {0.8}{{0.2}} = {4}}$\n",
    "\n",
    "the odd of team india winning the match is 4 to 1\n",
    "\n",
    "Final equation,\n",
    "\n",
    "${log(\\frac {P}{{1} - {P}}) = {\\beta_0} + {\\beta_1x}}$\n",
    "\n",
    "If `x` is increased by 1 unit, log odds will be increased by $\\beta_1$ units\n",
    "\n",
    "The odds of y = 1 will be increased over y = 0\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "<img src=\"cost-func-log-reg.png\" alt=\"Sample OLS Model\" style=\"width: 400px;float:center;\"></img>\n",
    "\n",
    "likelihood (cost function) = $({1} - {P_1})({1} - {P_2})({1} - {P_3})({1} - {P_4})({P_5})({1} - {P_6})({P_7})({P_8})({P_9})({P_{10}})$\n",
    "\n",
    "max likelihood = $log[({1} - {P_1})({1} - {P_2})({1} - {P_3})({1} - {P_4})({P_5})({1} - {P_6})({P_7})({P_8})({P_9})({P_{10}})]$\n",
    "\n",
    "max likelihood = $log({1} - {P_1}) + log({1} - {P_2}) + log({1} - {P_3}) + log({1} - {P_4}) + log({P_5}) + log({1} - {P_6}) + log({P_7}) + log({P_8}) + log({P_9}) + log({P_{10}})$\n",
    "\n",
    "max likelihood = $max[\\sum_{i=1}^m{log({1} - {P_i})} + \\sum_{i={m+1}}^n{log({P_i})}]$\n",
    "\n",
    "\n",
    "### Model Evaluation\n",
    "* Accuracy\n",
    "* Confusion Matrix: <i>\"the confusion matrix shows the ways in which your classification model is confused when it makes prediction\"</i>\n",
    "\n",
    "<img src=\"confusion-matrix.png\" alt=\"Sample OLS Model\" style=\"width: 200px;float:center;\"></img>\n",
    "\n",
    "|Actual/Predicted (COVID-19)|Infected|Not Infected|\n",
    "|---|---|---|\n",
    "|Infected|100|100|\n",
    "|Not Infected|5|900|\n",
    "\n",
    "$$Accuracy = \\frac {TP + TN} {TP + TN + FN + FP} = \\frac {100 + 900}{100 + 900 + 100 + 5} = \\frac {1000}{1105} = approx. 0.9$$\n",
    "\n",
    "However this might be miss leading in this case, let's check detection rate,\n",
    "\n",
    "$$DetectionRate = \\frac {TP}{TP + FN} = \\frac {100}{100 + 100} = \\frac {100}{200} = 0.5$$\n",
    "\n",
    "So this is a `bad model`, as model could detect only `50%` of times\n",
    "\n",
    "**Important Point:** `detection rate` is also called `recall/true positive rate/senstivity` and we do this when `FN` error is `costly`\n",
    "\n",
    "|Actual/Predicted (Predict Guilty)|Guilty|Not Guilty|\n",
    "|---|---|---|\n",
    "|Guilty|100|10|\n",
    "|Not Guilty|90|900|\n",
    "\n",
    "let's check `accuracy` score,\n",
    "\n",
    "$$Accuracy = \\frac {TP + TN} {TP + TN + FN + FP} = \\frac {100 + 900}{100 + 900 + 100 + 5} = \\frac {1000}{1105} = approx. 0.9$$\n",
    "\n",
    "let's check `recall` score,\n",
    "\n",
    "$$DetectionRate = \\frac {TP}{TP + FN} = \\frac {100}{100 + 10} = \\frac {100}{110} = 0.9$$\n",
    "\n",
    "Awesome, this is the best model but wait let's see which error is costly here, before moving ahead all over world judicial system follow this moto `let 100 guilty people go free but even single innocent people should be put in jail`\n",
    "\n",
    "If we see the matrix, there were `110` guilty people, `10` people who were guilty we set them free which is okay however there were `990` not guilty people actually out of which we predicted `900` not guilty but still `90` innocent people we sent to jail\n",
    "\n",
    "So here `recall` won't be good matrix, we use `precision` score here,\n",
    "\n",
    "$$Precision = \\frac {TP}{TP + FP} = \\frac {100}{100 + 90} = \\frac {100}{190} = approx. 0.55$$\n",
    "\n",
    "So this is a `bad model`\n",
    "\n",
    "**Important Point:** when `FP` error is `costly`, we go by `precision`\n",
    "\n",
    "**Important Point:** when both `recall` and `precision` is important, we use `f1 score`,\n",
    "\n",
    "$$f1 score = \\frac {2PR}{P+R}$$\n",
    "\n",
    "where,\n",
    "* P - Precision\n",
    "* R - Recall\n",
    "\n",
    "**Important Point:** For `precision` and `recall`, each is the true positive (TP) as the numerator divided by a different denominator.\n",
    "\n",
    "* Precision: TP / Predicted positive (FP) i.e. $\\frac {TP}{TP + FP}$\n",
    "* Recall: TP / Real positive (FN) i.e. $\\frac {TP}{TP + FN}$\n",
    "\n",
    "### ROC Curve\n",
    "\n",
    "* ROC curve summarize the trade-off between the `true positive rate` or `recall` and `false positive rate` or `precision` for a predictive model using different probability thresholds.\n",
    "* It's often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positive)\n",
    "\n",
    "<img src=\"roc-auc-curve.png\" style=\"width: 300px;float:center;\"></img>\n",
    "\n",
    "$$TPR/Recall/Sensitivity = \\frac {TP}{TP + FN}$$\n",
    "\n",
    "$$Specificity = \\frac {TN}{TN + FP}$$\n",
    "\n",
    "**Important Point:** We need to maximize `true positive rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f46ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clustering\n",
    "* Unsupervised Learning\n",
    "* K-Means clustering (most commly used clustering): centroids based algorithm\n",
    "\n",
    "#### K-Mean Clustering Steps\n",
    "1. Choose the number of clusters `k`\n",
    "2. Select `k` random points from the data as centroids\n",
    "3. Assign all the points to the closest cluster centroid\n",
    "4. Recompute the centroids of newly formed clusters\n",
    "5. Repeat steps `3` and `4`\n",
    "\n",
    "#### K-Mean Clustering - Stopping Criteria\n",
    "* Centroids of newly formed clusters do not change\n",
    "* Points remain in the same cluster\n",
    "* Maximum number of iterations are reached\n",
    "\n",
    "**Important Points:** Euclidean Distance between the 2 points, if there are 2 points `X` and `Y` having `n dimensions`\n",
    "\n",
    "$X = ({X_1}, {X_2}, {X_3}, {X_4}, ..., {X_n})$\n",
    "\n",
    "$Y = ({Y_1}, {Y_2}, {Y_3}, {Y_4}, ..., {Y_n})$\n",
    "\n",
    "Then the `Euclidean Distance D` is given as,\n",
    "\n",
    "$D = \\sqrt{{({X_1} - {Y_1})^2} + {({X_2} - {Y_2})^2} + {({X_3} - {Y_3})^2} + ... + {({X_n} - {Y_n})^2}}$\n",
    "\n",
    "\n",
    "#### Hopkins Score\n",
    "* Used to check cluster tendency, basically we check if data is having some visible clusters or not\n",
    "    * `0.3 score` - data is not good for cluster\n",
    "    * `score >= .7` - data is good for cluster\n",
    "    \n",
    "#### Silhoutte Score\n",
    "* It helps to find out the quality of clusters\n",
    "* It finds homogenity of clusters within and heterogenity among clusters\n",
    "\n",
    "\n",
    "The range of Silhouette score is [-1, 1]. Its analysis is as follows −\n",
    "\n",
    "* +1 Score − Near +1 Silhouette score indicates that the sample is far away from its neighboring cluster.\n",
    "* 0 Score − 0 Silhouette score indicates that the sample is on or very close to the decision boundary separating two neighboring clusters.\n",
    "* -1 Score − 1 Silhouette score indicates that the samples have been assigned to the wrong clusters.\n",
    "\n",
    "The calculation of Silhouette score can be done by using the following formula\n",
    "\n",
    "$$SilhouetteScore = \\frac {p−q}{max(p,q)}$$\n",
    "\n",
    "where, \n",
    "* p = mean distance to the points in the nearest cluster\n",
    "* q = mean intra-cluster distance to all the points\n",
    "\n",
    "\n",
    "**Important Points:** we should know about `dendogram`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1134a7",
   "metadata": {},
   "source": [
    "## Python Coding Quesrtions Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44113d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_list = [1,2,3,4,5,3,2,6,8,9,2,3,5]\n",
    "sorted_single_elements_list = list(reversed(list(set(inp_list))))\n",
    "sorted_single_elements_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa347e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_largest_number(inp_list, n):\n",
    "    return 'None' if ((len(inp_list) == 0) and (len(inp_list) < n)) else list(reversed(list(set(inp_list))))[n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe7f6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nth_largest_number(inp_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1dbe7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nth_largest_number([], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb177b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_number(num_list, nth_number):\n",
    "    return sorted(set(inp_list), reverse=True)[nth_number-1] if (len(num_list) !=0) and (nth_number <= len(num_list)) else 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1600836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_number(inp_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "501e66d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(inp_list), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1018b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b')\n"
     ]
    }
   ],
   "source": [
    "a = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\n",
    "x = slice(2)\n",
    "print(a[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
